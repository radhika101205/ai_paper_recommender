{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section Presence: {'abstract': 1, 'methodology': 1, 'results': 1}\n",
      "Scientific Rigor Score (1 = Complete, 0 = Incomplete): 1\n"
     ]
    }
   ],
   "source": [
    "# Section check\n",
    "\n",
    "import PyPDF2\n",
    "import re\n",
    "\n",
    "SECTION_SYNONYMS = {\n",
    "    \"abstract\": [\"abstract\", \"summary\"],\n",
    "    \"methodology\": [\"methodology\", \"approach\", \"methods\", \"experimental setup\", \"methodologies\", \"method\"],\n",
    "    \"results\": [\"results\", \"findings\", \"outcome\", \"discussion\", \"conclusions\", \"conclusion\"],\n",
    "}\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            text = [page.extract_text() for page in reader.pages]\n",
    "        return \"\\n\".join(text)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def check_section_presence(text, section_synonyms):\n",
    "    \n",
    "    section_presence = {section: 0 for section in section_synonyms}\n",
    "    for section, synonyms in section_synonyms.items():\n",
    "        # Check if any synonym for the section exists in the text\n",
    "        if any(re.search(rf\"\\b{synonym}\\b\", text, re.IGNORECASE) for synonym in synonyms):\n",
    "            section_presence[section] = 1\n",
    "    return section_presence\n",
    "\n",
    "def evaluate_scientific_rigor(section_presence):\n",
    "    \n",
    "    if all(value == 1 for value in section_presence.values()):\n",
    "        return 1  # Complete\n",
    "    return 0  # Incomplete\n",
    "\n",
    "pdf_path = r\"C:\\Users\\Radhika\\Downloads\\labelled papers\\R002.pdf\"\n",
    "text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# Check for the presence of sections\n",
    "section_presence = check_section_presence(text, SECTION_SYNONYMS)\n",
    "\n",
    "# Evaluate scientific rigor\n",
    "rigor_score = evaluate_scientific_rigor(section_presence)\n",
    "\n",
    "# Output results\n",
    "print(\"Section Presence:\", section_presence)\n",
    "print(\"Scientific Rigor Score (1 = Complete, 0 = Incomplete):\", rigor_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Readability Scores:\n",
      "Flesch Reading Ease: 12.03\n",
      "Gunning Fog Index: 19.22\n",
      "Automated Readability Index: 26.5\n",
      "Flesch Reading Ease Result: Fail\n"
     ]
    }
   ],
   "source": [
    "#Section check\n",
    "\n",
    "import textstat\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text)  \n",
    "    return text.strip()\n",
    "\n",
    "def calculate_readability_scores(text):\n",
    "    \n",
    "    scores = {\n",
    "        \"flesch_reading_ease\": textstat.flesch_reading_ease(text),\n",
    "        \"gunning_fog\": textstat.gunning_fog(text),\n",
    "        \"automated_readability_index\": textstat.automated_readability_index(text),\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "def evaluate_readability_threshold(score, threshold=50):\n",
    "    \n",
    "    return \"Pass\" if score >= threshold else \"Fail\"\n",
    "\n",
    "\n",
    "pdf_path = r\"C:\\Users\\Radhika\\Downloads\\labelled papers\\R002.pdf\"  \n",
    "\n",
    "# Step 1: Extract text from PDF\n",
    "text = extract_text_from_pdf(pdf_path)  \n",
    "text = clean_text(text)\n",
    "\n",
    "# Step 2: Calculate readability scores\n",
    "readability_scores = calculate_readability_scores(text)\n",
    "\n",
    "# Step 3: Evaluate readability against the threshold\n",
    "flesch_result = evaluate_readability_threshold(readability_scores[\"flesch_reading_ease\"])\n",
    "\n",
    "# Output results\n",
    "print(\"Readability Scores:\")\n",
    "print(f\"Flesch Reading Ease: {readability_scores['flesch_reading_ease']}\")\n",
    "print(f\"Gunning Fog Index: {readability_scores['gunning_fog']}\")\n",
    "print(f\"Automated Readability Index: {readability_scores['automated_readability_index']}\")\n",
    "print(f\"Flesch Reading Ease Result: {flesch_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import re\n",
    "import PyPDF2\n",
    "\n",
    "# Load a pre-trained model for sentence embeddings\n",
    "nlp = pipeline(\"feature-extraction\", model=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def extract_paragraphs_from_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Extract paragraphs from a PDF file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            reader = PyPDF2.PdfReader(f)\n",
    "            text = [page.extract_text() for page in reader.pages]\n",
    "        # Join all text and split into paragraphs\n",
    "        paragraphs = \"\\n\".join(text).split(\"\\n\\n\")\n",
    "        return [para.strip() for para in paragraphs if para.strip()]\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return []\n",
    "\n",
    "def split_into_sentences(paragraph):\n",
    "    \"\"\"\n",
    "    Split a paragraph into sentences using regex.\n",
    "    \"\"\"\n",
    "    return re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', paragraph)\n",
    "\n",
    "def evaluate_pdf_content(pdf_path):\n",
    "    \"\"\"\n",
    "    Process paragraphs and sentences in a PDF.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): Path to the input PDF.\n",
    "    \n",
    "    Returns:\n",
    "        list: Extracted paragraphs with sentences split.\n",
    "    \"\"\"\n",
    "    paragraphs = extract_paragraphs_from_pdf(pdf_path)\n",
    "    processed_content = []\n",
    "    \n",
    "    for i, paragraph in enumerate(paragraphs):\n",
    "        sentences = split_into_sentences(paragraph)\n",
    "        processed_content.append({\n",
    "            \"Paragraph\": f\"Paragraph {i + 1}\",\n",
    "            \"Sentences\": sentences\n",
    "        })\n",
    "\n",
    "    return processed_content\n",
    "\n",
    "# Example Usage\n",
    "pdf_path = r\"C:\\Users\\Radhika\\Downloads\\labelled papers\\R002.pdf\"  # Replace with the path to your PDF\n",
    "processed_content = evaluate_pdf_content(pdf_path)\n",
    "\n",
    "# Output processed content\n",
    "print(\"Processed PDF Content:\")\n",
    "for content in processed_content:\n",
    "    print(f\"{content['Paragraph']}:\")\n",
    "    for sentence in content[\"Sentences\"]:\n",
    "        print(f\"  - {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Sample Data (replace this with your actual dataset)\n",
    "# Each row represents a paper with features (scores from checks) and label (publishable or not)\n",
    "data = {\n",
    "    \"section_check\": [0.9, 0.85, 0.7, 0.95, 0.4, 0.6, 0.8, 0.9, 0.7, 0.5, 0.65, 0.75, 0.88, 0.92, 0.3],\n",
    "    \"readability_score\": [0.8, 0.82, 0.75, 0.9, 0.4, 0.6, 0.78, 0.88, 0.65, 0.5, 0.6, 0.7, 0.85, 0.91, 0.35],\n",
    "    \"coherence_score\": [0.85, 0.87, 0.72, 0.93, 0.42, 0.62, 0.81, 0.89, 0.68, 0.55, 0.67, 0.78, 0.86, 0.94, 0.4],\n",
    "    \"novelty_score\": [0.92, 0.88, 0.8, 0.96, 0.45, 0.65, 0.83, 0.9, 0.7, 0.52, 0.63, 0.74, 0.9, 0.95, 0.38],\n",
    "    \"label\": [1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0],  # 1: Publishable, 0: Not Publishable\n",
    "}\n",
    "\n",
    "# Convert data to a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = df.drop(\"label\", axis=1)\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Print Results\n",
    "print(\"Accuracy of Logistic Regression Model:\", accuracy)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Optional: Check model coefficients to understand feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Coefficient\": model.coef_[0]\n",
    "}).sort_values(by=\"Coefficient\", ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
